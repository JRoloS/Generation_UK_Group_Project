{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Variables used for testing\n",
    "import pandas as pd                                                                                 # Loads Pandas package \n",
    "import os                                                                                           # For OS related paths \n",
    "\n",
    "# Variable List (with their default value [Used in Testing])\n",
    "raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"                                              # Csv filename on the raw_data subfolder\n",
    "csv = \"\"                                                                                            # filepath to be read by functions, used after joint with raw_data_file NOTE: csv variable could be an url if needed\n",
    "df = \"\"                                                                                             # Global for the DataFrame\n",
    "count = \"\"                                                                                          # Global for count functions\n",
    "column = \"payment_type\"                                                                             # Argument for functions\n",
    "value = \"CARD\"                                                                                      # Argument for functions\n",
    "columns = ['date_time', 'Location', 'full_name', 'order', 'transaction_total', 'payment_type', 'card_number']  # Headers for the orders csv files\n",
    "sanitise_these_columns = ['full_name', 'card_number']                                               # Columns list to be sanitised\n",
    "path = \"results/\"                                                                                   # to add folder/subfolder/ if needed\n",
    "newfile = \"newfile.csv\"                                                                             # Filename to be created by the Create CSV/JSON function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ABSOLUTE PATH FOR RAW DATA   \n",
    "\n",
    "# takes: raw_data_name that should be the name of one of the csv's files \n",
    "# returns: absolute path to the raw_data subfolder folder plus raw_data_name AS a variable named csv \n",
    "\n",
    "\n",
    "def absolute_path_for_raw_data(raw_data_file):\n",
    "    abspath = os.path.abspath(\"../raw_data\")\n",
    "    global csv\n",
    "    csv = abspath + \"\\\\\" + raw_data_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jzzz\\Python\\final-project\\brewed-awakening-final-project\\raw_data\\leeds_01-01-2020_09-00-00.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. [TEST RUN] ABSOLUTE PATH FOR RAW DATA\n",
    "\n",
    "# Current Files are Chesterfield or Leeds (comment one out)\n",
    "# raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "absolute_path_for_raw_data(raw_data_file)\n",
    "print(csv) # Prints the newly formed absolute path with the raw_data_file name given.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. EXTRACT SANITISE CSV (Remove given Columns) \n",
    "\n",
    "# takes: csv(absolute path to filename), and a list of columns to be dropped on a variable called sanatise_these_columns\n",
    "# returns: the sanitised dataframe as df\n",
    "\n",
    "def extract_sanitise_csv(csv,sanitise_these_columns):\n",
    "    try:\n",
    "        global df # as Global to be able to print it outside the function\n",
    "        df = pd.read_csv(csv, header=None, names=columns)\n",
    "        df = df.drop(columns=sanitise_these_columns)\n",
    "    except FileNotFoundError as fnfe:\n",
    "        print(f'File not found: {fnfe}')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. [TEST RUN] EXTRACT SANITISE CSV \n",
    "\n",
    "# Current Files Chesterfield or Leeds (comment one out)\n",
    "# raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "absolute_path_for_raw_data(raw_data_file)  # returns csv variable with the absolute path\n",
    " \n",
    "sanitise_these_columns = ['full_name', 'card_number']\n",
    "\n",
    "extract_sanitise_csv(csv,sanitise_these_columns)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. FILTER BY COLUMN VALUE \n",
    "\n",
    "# Takes: csv(absolute path to filename), column variable with the column header id, value to check\n",
    "# Returns: the rows in which a column contains a given value AS contain_values\n",
    "\n",
    "def filter_by_column_value(csv,column,value):\n",
    "    try:\n",
    "        global contain_values # as Global to be able to print it outside the function\n",
    "        contain_values = df[df[column].str.contains(value.upper())]\n",
    "    except FileNotFoundError as fnfe:\n",
    "        print(f'File not found: {fnfe}')\n",
    "\n",
    "    return contain_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>Location</th>\n",
       "      <th>order</th>\n",
       "      <th>transaction_total</th>\n",
       "      <th>payment_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/01/2020 09:01</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Large Chai latte - 2.60, Regular Filter coffee...</td>\n",
       "      <td>4.10</td>\n",
       "      <td>CARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/01/2020 09:03</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Large Speciality Tea - English breakfast - 1.6...</td>\n",
       "      <td>2.90</td>\n",
       "      <td>CARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/01/2020 09:04</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Large Chai latte - 2.60, Large Iced americano ...</td>\n",
       "      <td>7.60</td>\n",
       "      <td>CARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/01/2020 09:06</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Regular Hot Chocolate - 1.40</td>\n",
       "      <td>1.40</td>\n",
       "      <td>CARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>01/01/2020 09:08</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Regular Chai latte - 2.30, Regular Filter coff...</td>\n",
       "      <td>3.80</td>\n",
       "      <td>CARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>01/01/2020 09:09</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Regular Iced americano - 2.15, Regular Hot Cho...</td>\n",
       "      <td>10.75</td>\n",
       "      <td>CARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>01/01/2020 09:11</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Regular Filter coffee - 1.50, Regular Hot Choc...</td>\n",
       "      <td>8.40</td>\n",
       "      <td>CARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>01/01/2020 09:12</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Large Iced americano - 2.50, Regular Chai latt...</td>\n",
       "      <td>4.80</td>\n",
       "      <td>CARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>01/01/2020 09:19</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Regular Hot Chocolate - 1.40, Large Filter cof...</td>\n",
       "      <td>6.85</td>\n",
       "      <td>CARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>01/01/2020 09:21</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Large Speciality Tea - English breakfast - 1.6...</td>\n",
       "      <td>4.10</td>\n",
       "      <td>CARD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date_time Location   \n",
       "1   01/01/2020 09:01    Leeds  \\\n",
       "2   01/01/2020 09:03    Leeds   \n",
       "3   01/01/2020 09:04    Leeds   \n",
       "4   01/01/2020 09:06    Leeds   \n",
       "5   01/01/2020 09:08    Leeds   \n",
       "6   01/01/2020 09:09    Leeds   \n",
       "7   01/01/2020 09:11    Leeds   \n",
       "8   01/01/2020 09:12    Leeds   \n",
       "12  01/01/2020 09:19    Leeds   \n",
       "13  01/01/2020 09:21    Leeds   \n",
       "\n",
       "                                                order  transaction_total   \n",
       "1   Large Chai latte - 2.60, Regular Filter coffee...               4.10  \\\n",
       "2   Large Speciality Tea - English breakfast - 1.6...               2.90   \n",
       "3   Large Chai latte - 2.60, Large Iced americano ...               7.60   \n",
       "4                        Regular Hot Chocolate - 1.40               1.40   \n",
       "5   Regular Chai latte - 2.30, Regular Filter coff...               3.80   \n",
       "6   Regular Iced americano - 2.15, Regular Hot Cho...              10.75   \n",
       "7   Regular Filter coffee - 1.50, Regular Hot Choc...               8.40   \n",
       "8   Large Iced americano - 2.50, Regular Chai latt...               4.80   \n",
       "12  Regular Hot Chocolate - 1.40, Large Filter cof...               6.85   \n",
       "13  Large Speciality Tea - English breakfast - 1.6...               4.10   \n",
       "\n",
       "   payment_type  \n",
       "1          CARD  \n",
       "2          CARD  \n",
       "3          CARD  \n",
       "4          CARD  \n",
       "5          CARD  \n",
       "6          CARD  \n",
       "7          CARD  \n",
       "8          CARD  \n",
       "12         CARD  \n",
       "13         CARD  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. [TEST RUN] FILTER BY COLUMN VALUE\n",
    "\n",
    "# Current Files Chesterfield or Leeds (comment one out)\n",
    "raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "# raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "column = \"payment_type\"  \n",
    "value = \"card\"\n",
    "\n",
    "filter_by_column_value(csv,column,value)\n",
    "contain_values.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. COUNT NUMBER OF DIFFERENT VALUES IN COLUMN \n",
    "\n",
    "# Takes: csv(absolute path to filename), column variable with the column header id,\n",
    "# Returns: the count of the different values contained in a given column\n",
    "\n",
    "def count_number_of_different_values(csv,column):\n",
    "    try:\n",
    "        global count  # as Global to be able to print it outside the function\n",
    "        count = df[column].value_counts(ascending=True)\n",
    "    except FileNotFoundError as fnfe:\n",
    "         print(f'File not found: {fnfe}')\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payment_type\n",
      "CASH    107\n",
      "CARD    275\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 4. [TEST RUN] COUNT NUMBER OF DIFFERENT VALUES IN COLUMN \n",
    "\n",
    "# Current Files Chesterfield or Leeds (comment one out)\n",
    "# raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "absolute_path_for_raw_data(raw_data_file) # returns csv variable with the absolut path\n",
    "\n",
    "column = \"payment_type\" \n",
    "\n",
    "count_number_of_different_values(csv,column)\n",
    "print(str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. COUNT NUMBER OF TIMES A VALUE IS REPEATED\n",
    "# \n",
    "# Takes: csv(absolute path to filename), column variable with the column header id, value to check\n",
    "# Retuns: the count of the number of times a single value is repeated on a given column AS count\n",
    "\n",
    "def count_number_of_times_a_value_is_repeated(csv,column,value):\n",
    "    try:\n",
    "        df = pd.read_csv(csv, header=None, names=columns)\n",
    "        global count # as Global to be able to print it outside the function\n",
    "        count = df[column].value_counts()[value]\n",
    "    except FileNotFoundError as fnfe:\n",
    "         print(f'File not found: {fnfe}')\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the value: CARD was found 275 times.\n"
     ]
    }
   ],
   "source": [
    "# 5. [TEST RUN] COUNT NUMBER OF TIMES A VALUE IS REPEATED\n",
    "\n",
    "# Current Files Chesterfield or Leeds (comment one out)\n",
    "# raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "absolute_path_for_raw_data(raw_data_file) # returns csv variable with the absolut path\n",
    "\n",
    "column = \"payment_type\"\n",
    "value = \"CARD\" \n",
    "\n",
    "count_number_of_times_a_value_is_repeated(csv,column,value)\n",
    "print(\"the value: \" + value + \" was found \" + str(count) + \" times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. SAVE DATA FRAME TO FILE AS CSV\n",
    "\n",
    "# Takes: path for the desired save path, newfile for the desired new file name (.csv extension added at runtime)\n",
    "# Retuns: a csv file saved to the desired location that contains the DF\n",
    "\n",
    "def save_df_to_csv(path,newfile):\n",
    "    try:\n",
    "        os.makedirs(path, exist_ok=True)  \n",
    "        df.to_csv(path + newfile +\".csv\", header=False)\n",
    "\n",
    "    except:\n",
    "        print(f'Saving operation could not be completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. [TEST RUN] SAVE DATA FRAME TO FILE AS CSV\n",
    "path = \"results/\" # to add folder/subfolder/ if needed\n",
    "newfile = \"newfile\"\n",
    "\n",
    "save_df_to_csv(path,newfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. SAVE DATAFRAME TO FILE AS JSON\n",
    "\n",
    "# Takes: path for the desired save path, newfile for the desired new file name (.json extension added at runtime)\n",
    "# Retuns: a csv file saved to the desired location that contains the DF\n",
    "\n",
    "def save_df_to_json(path,newfile):\n",
    "    try:\n",
    "        with open(path + newfile +\".json\", 'w') as f:\n",
    "            f.write(df.to_json(orient='records', lines=True))  # add compression='gzip' to get a zip file (and change newfile extension)\n",
    "\n",
    "    except:\n",
    "        print(f'Saving operation could not be completed')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. [TEST RUN] SAVE DATAFRAME TO FILE AS JSON\n",
    "\n",
    "path = \"results/\" # to add folder/subfolder/ if needed\n",
    "newfile = \"newfile\"\n",
    "\n",
    "save_df_to_json(path,newfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GET UNIQUE PRODUCTS IN ORDERS [WORK IN PROGRESS]\n",
    "\n",
    "\n",
    "def get_unique_products_in_orders():\n",
    "    df = pd.read_csv(csv, header=None, names=columns)\n",
    "    orders = df['order'].str.split(',', expand = True)\n",
    "        \n",
    "    for item in orders:\n",
    "        global result\n",
    "        result = orders[item].drop_duplicates()\n",
    "\n",
    "    return result\n",
    "\n",
    "# TODO: we need to separate prices from the values\n",
    "# but we can't just use a split('-') because some products like the [Large Flavoured latte - Hazelnut - 2.85] will break the name - price norm. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                   None\n",
       "50                         Regular Iced americano - 2.15\n",
       "55                               Large Chai latte - 2.60\n",
       "57      Regular Speciality Tea - English breakfast - ...\n",
       "122      Large Speciality Tea - English breakfast - 1.60\n",
       "142                          Large Iced americano - 2.50\n",
       "154                            Regular Chai latte - 2.30\n",
       "237                         Regular Filter coffee - 1.50\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [TEST RUN] GET UNIQUE PRODUCTS IN ORDERS [WORK IN PROGRESS]\n",
    "\n",
    "# Current Files Chesterfield or Leeds (comment one out)\n",
    "# raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "absolute_path_for_raw_data(raw_data_file) # returns csv variable with the absolut path\n",
    "result = \"\"\n",
    "\n",
    "\n",
    "get_unique_products_in_orders()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.30\n",
       "1      1.50\n",
       "2      1.30\n",
       "3      2.50\n",
       "4      1.40\n",
       "5      1.50\n",
       "6      2.30\n",
       "7      1.50\n",
       "8      2.30\n",
       "9      2.30\n",
       "10     2.15\n",
       "11     2.30\n",
       "12     2.15\n",
       "13     2.50\n",
       "14     2.50\n",
       "15     1.70\n",
       "16     1.70\n",
       "17     1.70\n",
       "18     2.15\n",
       "19     2.60\n",
       "20     2.50\n",
       "21     1.80\n",
       "22     1.50\n",
       "23     1.50\n",
       "24     1.80\n",
       "25     1.70\n",
       "26     2.15\n",
       "27     1.50\n",
       "28     1.80\n",
       "29     1.30\n",
       "30     2.15\n",
       "31     2.15\n",
       "32     2.30\n",
       "33     1.40\n",
       "34     1.30\n",
       "35     1.80\n",
       "36     2.50\n",
       "37     1.70\n",
       "38     1.30\n",
       "39     1.50\n",
       "40     1.80\n",
       "41     2.50\n",
       "42     2.15\n",
       "43     1.40\n",
       "44     1.60\n",
       "45     1.80\n",
       "46     1.80\n",
       "47     1.30\n",
       "48     1.30\n",
       "49     1.80\n",
       "50     2.15\n",
       "51     1.50\n",
       "52     2.50\n",
       "53     2.50\n",
       "54     1.70\n",
       "55     2.60\n",
       "56     2.15\n",
       "57     1.30\n",
       "58     2.15\n",
       "59     1.40\n",
       "60     1.40\n",
       "61     1.30\n",
       "62     1.30\n",
       "63     1.80\n",
       "64     1.50\n",
       "65     1.30\n",
       "66     2.50\n",
       "67     1.30\n",
       "68     2.60\n",
       "69     1.40\n",
       "70     1.50\n",
       "71     2.30\n",
       "72     1.50\n",
       "73     1.80\n",
       "74     1.80\n",
       "75     1.30\n",
       "76     1.80\n",
       "77     2.15\n",
       "78     2.15\n",
       "79     1.70\n",
       "80     1.70\n",
       "81     2.60\n",
       "82     1.60\n",
       "83     2.60\n",
       "84     1.70\n",
       "85     1.60\n",
       "86     1.30\n",
       "87     2.50\n",
       "88     1.80\n",
       "89     1.70\n",
       "90     1.80\n",
       "91     2.50\n",
       "92     1.30\n",
       "93     1.60\n",
       "94     1.70\n",
       "95     2.60\n",
       "96     1.80\n",
       "97     2.30\n",
       "98     2.15\n",
       "99     1.40\n",
       "100    2.30\n",
       "101    1.70\n",
       "102    1.80\n",
       "103    2.15\n",
       "104    1.80\n",
       "105    2.30\n",
       "106    2.50\n",
       "107    1.70\n",
       "108    2.60\n",
       "109    2.60\n",
       "110    1.30\n",
       "111    1.80\n",
       "112    2.30\n",
       "113    1.50\n",
       "114    2.60\n",
       "115    2.50\n",
       "116    1.70\n",
       "117    1.70\n",
       "118    2.50\n",
       "119    1.80\n",
       "120    1.50\n",
       "121    1.40\n",
       "122    1.60\n",
       "123    1.50\n",
       "124    2.15\n",
       "125    2.60\n",
       "126    1.30\n",
       "127    2.50\n",
       "128    1.70\n",
       "129    1.60\n",
       "130    2.60\n",
       "131    1.30\n",
       "132    2.50\n",
       "133    1.40\n",
       "134    1.30\n",
       "135    1.50\n",
       "136    1.80\n",
       "137    2.15\n",
       "138    2.15\n",
       "139    2.30\n",
       "140    2.15\n",
       "141    2.50\n",
       "142    2.50\n",
       "143    2.30\n",
       "144    1.60\n",
       "145    1.40\n",
       "146    2.60\n",
       "147    2.30\n",
       "148    2.15\n",
       "149    2.50\n",
       "150    1.40\n",
       "151    2.15\n",
       "152    1.50\n",
       "153    1.70\n",
       "154    2.30\n",
       "155    1.70\n",
       "156    1.40\n",
       "157    1.80\n",
       "158    1.30\n",
       "159    1.30\n",
       "160    2.30\n",
       "161    1.60\n",
       "162    2.30\n",
       "163    2.15\n",
       "164    2.15\n",
       "165    1.80\n",
       "166    1.60\n",
       "167    1.80\n",
       "168    1.40\n",
       "169    1.40\n",
       "170    1.80\n",
       "171    1.50\n",
       "172    2.50\n",
       "173    2.60\n",
       "174    1.50\n",
       "175    2.60\n",
       "176    1.40\n",
       "177    2.30\n",
       "178    2.50\n",
       "179    2.15\n",
       "180    1.60\n",
       "181    1.40\n",
       "182    2.50\n",
       "183    2.30\n",
       "184    1.40\n",
       "185    1.30\n",
       "186    1.60\n",
       "187    1.50\n",
       "188    1.80\n",
       "189    2.50\n",
       "190    1.30\n",
       "191    1.70\n",
       "192    1.50\n",
       "193    1.30\n",
       "194    1.40\n",
       "195    2.30\n",
       "196    1.80\n",
       "197    1.60\n",
       "198    2.15\n",
       "199    1.40\n",
       "200    1.80\n",
       "201    2.30\n",
       "202    2.50\n",
       "203    1.70\n",
       "204    1.60\n",
       "205    1.50\n",
       "206    1.80\n",
       "207    1.40\n",
       "208    2.50\n",
       "209    1.70\n",
       "210    2.60\n",
       "211    1.60\n",
       "212    2.30\n",
       "213    1.60\n",
       "214    1.80\n",
       "215    1.70\n",
       "216    1.50\n",
       "217    2.60\n",
       "218    1.40\n",
       "219    2.60\n",
       "220    1.30\n",
       "221    1.40\n",
       "222    2.50\n",
       "223    1.70\n",
       "224    1.80\n",
       "225    1.70\n",
       "226    1.70\n",
       "227    2.50\n",
       "228    1.70\n",
       "229    2.30\n",
       "230    2.50\n",
       "231    1.30\n",
       "232    2.60\n",
       "233    1.70\n",
       "234    2.30\n",
       "235    1.70\n",
       "236    1.80\n",
       "237    1.50\n",
       "238    1.70\n",
       "239    1.50\n",
       "240    2.30\n",
       "241    1.50\n",
       "242    1.50\n",
       "243    1.80\n",
       "244    2.60\n",
       "245    1.60\n",
       "246    2.50\n",
       "247    1.50\n",
       "248    2.60\n",
       "249    2.50\n",
       "250    1.80\n",
       "251    2.60\n",
       "252    2.15\n",
       "253    2.60\n",
       "254    1.50\n",
       "255    1.50\n",
       "256    1.60\n",
       "257    1.40\n",
       "258    1.80\n",
       "259    1.80\n",
       "260    1.80\n",
       "261    2.60\n",
       "262    2.30\n",
       "263    1.60\n",
       "264    1.70\n",
       "265    1.80\n",
       "266    1.30\n",
       "267    1.70\n",
       "268    1.60\n",
       "269    1.50\n",
       "270    1.30\n",
       "271    2.50\n",
       "272    1.60\n",
       "273    2.30\n",
       "274    2.60\n",
       "275    1.50\n",
       "276    1.40\n",
       "277    2.60\n",
       "278    1.80\n",
       "279    1.70\n",
       "280    1.50\n",
       "281    2.15\n",
       "282    2.60\n",
       "283    2.15\n",
       "284    2.15\n",
       "285    1.40\n",
       "286    2.60\n",
       "287    1.60\n",
       "288    2.60\n",
       "289    1.60\n",
       "290    2.60\n",
       "291    1.30\n",
       "292    1.80\n",
       "293    1.70\n",
       "294    2.30\n",
       "295    2.30\n",
       "296    1.50\n",
       "297    2.30\n",
       "298    1.70\n",
       "299    2.30\n",
       "300    1.30\n",
       "301    2.30\n",
       "302    1.30\n",
       "303    1.70\n",
       "304    1.30\n",
       "305    2.15\n",
       "306    1.70\n",
       "307    1.70\n",
       "308    1.30\n",
       "309    2.50\n",
       "310    1.70\n",
       "311    1.30\n",
       "312    1.40\n",
       "313    1.50\n",
       "314    1.80\n",
       "315    2.30\n",
       "316    2.30\n",
       "317    1.40\n",
       "318    2.30\n",
       "319    1.60\n",
       "320    1.70\n",
       "321    1.80\n",
       "322    1.50\n",
       "323    2.60\n",
       "324    2.60\n",
       "325    1.40\n",
       "326    2.30\n",
       "327    1.80\n",
       "328    2.30\n",
       "329    2.30\n",
       "330    1.50\n",
       "331    1.50\n",
       "332    1.80\n",
       "333    1.80\n",
       "334    1.50\n",
       "335    1.40\n",
       "336    1.40\n",
       "337    1.40\n",
       "338    2.15\n",
       "339    1.80\n",
       "340    2.30\n",
       "341    1.50\n",
       "342    2.30\n",
       "343    2.50\n",
       "344    2.30\n",
       "345    1.60\n",
       "346    1.80\n",
       "347    2.30\n",
       "348    1.30\n",
       "349    1.40\n",
       "350    1.40\n",
       "351    1.60\n",
       "352    1.50\n",
       "353    2.60\n",
       "354    2.50\n",
       "355    2.30\n",
       "356    1.30\n",
       "357    2.60\n",
       "358    2.50\n",
       "359    2.50\n",
       "360    1.40\n",
       "361    1.70\n",
       "362    2.15\n",
       "363    2.30\n",
       "364    1.80\n",
       "365    2.15\n",
       "366    2.50\n",
       "367    2.30\n",
       "368    1.60\n",
       "369    1.60\n",
       "370    2.50\n",
       "371    1.60\n",
       "372    2.15\n",
       "373    2.60\n",
       "374    2.60\n",
       "375    1.70\n",
       "376    2.30\n",
       "377    1.60\n",
       "378    2.15\n",
       "379    1.50\n",
       "380    1.80\n",
       "381    1.60\n",
       "Name: order, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SHOWS PRICE ONLY [TESTING]\n",
    "df['order'].str.split(' ').str[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
