{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Pandas Functions\n",
    "\n",
    "Please run the imports first and keep in mind that some blocks require an absolute path csv and a DF loaded to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd                                                                                 # Loads Pandas package\n",
    "import os                                                                                           # For OS related paths "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. ABSOLUTE PATH FOR RAW DATA   \n",
    "\n",
    "##### Takes: raw_data_name that should be the name of one of the csv's files \n",
    "##### Returns: a variable named csv containing the absolute path to the raw_data subfolder folder plus the name of the file in raw_data_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ABSOLUTE PATH FOR RAW DATA   \n",
    "\n",
    "def absolute_path_for_raw_data(raw_data_file):\n",
    "    abspath = os.path.abspath(\"../raw_data\")\n",
    "    csv = abspath + \"\\\\\" + raw_data_file\n",
    "    return csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Python\\DELON9\\GitHubRepos\\brewed-awakening-group-project\\raw_data\\leeds_01-01-2020_09-00-00.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. [TEST RUN] ABSOLUTE PATH FOR RAW DATA\n",
    "\n",
    "# Current Files are Chesterfield or Leeds (comment one out)\n",
    "# raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "csv = absolute_path_for_raw_data(raw_data_file)\n",
    "print(csv) # Prints the newly formed absolute path with the raw_data_file name given.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #LOADS (EXTRACT) DATAFRAME\n",
    "\n",
    "#### Loads the df with headers .... just put outside any function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADS (EXTRACT) DATAFRAME\n",
    "columns = ['date_time', 'location', 'full_name', 'order', 'transaction_total', 'payment_type', 'card_number']  # Headers for the orders csv files\n",
    "df = pd.read_csv(csv, header=None, names=columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SANITISE CSV (Remove given Columns) \n",
    "\n",
    "#### Takes: csv(absolute path to filename), and a list of columns to be dropped on a variable called sanatise_these_columns\n",
    "#### Returns: the sanitised dataframe as df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SANITISE CSV (Remove given Columns) \n",
    "\n",
    "def sanitise_csv(csv,sanitise_these_columns):\n",
    "    try:\n",
    "        sanatisedf = df.drop(columns=sanitise_these_columns)\n",
    "    except FileNotFoundError as fnfe:\n",
    "        print(f'File not found: {fnfe}')\n",
    "\n",
    "    return sanatisedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>Location</th>\n",
       "      <th>order</th>\n",
       "      <th>amount</th>\n",
       "      <th>payment_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/2020 09:00</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Regular Chai latte - 2.30, Regular Speciality ...</td>\n",
       "      <td>3.6</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/01/2020 09:01</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Large Chai latte - 2.60, Regular Filter coffee...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>CARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/01/2020 09:03</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Large Speciality Tea - English breakfast - 1.6...</td>\n",
       "      <td>2.9</td>\n",
       "      <td>CARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/01/2020 09:04</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Large Chai latte - 2.60, Large Iced americano ...</td>\n",
       "      <td>7.6</td>\n",
       "      <td>CARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/01/2020 09:06</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Regular Hot Chocolate - 1.40</td>\n",
       "      <td>1.4</td>\n",
       "      <td>CARD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date_time Location   \n",
       "0  01/01/2020 09:00    Leeds  \\\n",
       "1  01/01/2020 09:01    Leeds   \n",
       "2  01/01/2020 09:03    Leeds   \n",
       "3  01/01/2020 09:04    Leeds   \n",
       "4  01/01/2020 09:06    Leeds   \n",
       "\n",
       "                                               order  amount payment_type  \n",
       "0  Regular Chai latte - 2.30, Regular Speciality ...     3.6         CASH  \n",
       "1  Large Chai latte - 2.60, Regular Filter coffee...     4.1         CARD  \n",
       "2  Large Speciality Tea - English breakfast - 1.6...     2.9         CARD  \n",
       "3  Large Chai latte - 2.60, Large Iced americano ...     7.6         CARD  \n",
       "4                       Regular Hot Chocolate - 1.40     1.4         CARD  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. [TEST RUN] SANITISE CSV \n",
    "\n",
    "# Current Files Chesterfield or Leeds (comment one out)\n",
    "# raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "columns = ['date_time', 'location', 'full_name', 'order', 'transaction_total', 'payment_type', 'card_number']  # Headers for the orders csv files\n",
    "\n",
    "absolute_path_for_raw_data(raw_data_file)  # returns csv variable with the absolute path\n",
    " \n",
    "sanitise_these_columns = ['full_name', 'card_number']\n",
    "\n",
    "result = sanitise_csv(csv,sanitise_these_columns)\n",
    "result.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. FILTER BY COLUMN VALUE \n",
    "\n",
    "#### Takes: csv(absolute path to filename), column variable with the column header id, value to check\n",
    "#### Returns: the rows in which a column contains a given value AS contain_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. FILTER BY COLUMN VALUE \n",
    "\n",
    "def filter_by_column_value(csv,column,value):\n",
    "    try:\n",
    "        contain_values = df[df[column].str.contains(value.upper())]\n",
    "    except FileNotFoundError as fnfe:\n",
    "        print(f'File not found: {fnfe}')\n",
    "\n",
    "    return contain_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>Location</th>\n",
       "      <th>full_name</th>\n",
       "      <th>order</th>\n",
       "      <th>amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>card_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/01/2020 09:01</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Matthew Palmer</td>\n",
       "      <td>Large Chai latte - 2.60, Regular Filter coffee...</td>\n",
       "      <td>4.10</td>\n",
       "      <td>CARD</td>\n",
       "      <td>5.933193e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/01/2020 09:03</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Mack Cendejas</td>\n",
       "      <td>Large Speciality Tea - English breakfast - 1.6...</td>\n",
       "      <td>2.90</td>\n",
       "      <td>CARD</td>\n",
       "      <td>2.728143e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/01/2020 09:04</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Thomas Williams</td>\n",
       "      <td>Large Chai latte - 2.60, Large Iced americano ...</td>\n",
       "      <td>7.60</td>\n",
       "      <td>CARD</td>\n",
       "      <td>2.992561e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/01/2020 09:06</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Diane Ferree</td>\n",
       "      <td>Regular Hot Chocolate - 1.40</td>\n",
       "      <td>1.40</td>\n",
       "      <td>CARD</td>\n",
       "      <td>8.040887e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>01/01/2020 09:08</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Bruce Watts</td>\n",
       "      <td>Regular Chai latte - 2.30, Regular Filter coff...</td>\n",
       "      <td>3.80</td>\n",
       "      <td>CARD</td>\n",
       "      <td>2.242609e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>01/01/2020 09:09</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Sergio Vanmeter</td>\n",
       "      <td>Regular Iced americano - 2.15, Regular Hot Cho...</td>\n",
       "      <td>10.75</td>\n",
       "      <td>CARD</td>\n",
       "      <td>1.548663e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>01/01/2020 09:11</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Rosemary Wertman</td>\n",
       "      <td>Regular Filter coffee - 1.50, Regular Hot Choc...</td>\n",
       "      <td>8.40</td>\n",
       "      <td>CARD</td>\n",
       "      <td>6.706720e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>01/01/2020 09:12</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Marcia Mason</td>\n",
       "      <td>Large Iced americano - 2.50, Regular Chai latt...</td>\n",
       "      <td>4.80</td>\n",
       "      <td>CARD</td>\n",
       "      <td>2.590703e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>01/01/2020 09:19</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Richard Redding</td>\n",
       "      <td>Regular Hot Chocolate - 1.40, Large Filter cof...</td>\n",
       "      <td>6.85</td>\n",
       "      <td>CARD</td>\n",
       "      <td>4.480989e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>01/01/2020 09:21</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Jean Wontor</td>\n",
       "      <td>Large Speciality Tea - English breakfast - 1.6...</td>\n",
       "      <td>4.10</td>\n",
       "      <td>CARD</td>\n",
       "      <td>3.200300e+15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date_time Location         full_name   \n",
       "1   01/01/2020 09:01    Leeds    Matthew Palmer  \\\n",
       "2   01/01/2020 09:03    Leeds     Mack Cendejas   \n",
       "3   01/01/2020 09:04    Leeds   Thomas Williams   \n",
       "4   01/01/2020 09:06    Leeds      Diane Ferree   \n",
       "5   01/01/2020 09:08    Leeds       Bruce Watts   \n",
       "6   01/01/2020 09:09    Leeds   Sergio Vanmeter   \n",
       "7   01/01/2020 09:11    Leeds  Rosemary Wertman   \n",
       "8   01/01/2020 09:12    Leeds      Marcia Mason   \n",
       "12  01/01/2020 09:19    Leeds   Richard Redding   \n",
       "13  01/01/2020 09:21    Leeds       Jean Wontor   \n",
       "\n",
       "                                                order  amount payment_type   \n",
       "1   Large Chai latte - 2.60, Regular Filter coffee...    4.10         CARD  \\\n",
       "2   Large Speciality Tea - English breakfast - 1.6...    2.90         CARD   \n",
       "3   Large Chai latte - 2.60, Large Iced americano ...    7.60         CARD   \n",
       "4                        Regular Hot Chocolate - 1.40    1.40         CARD   \n",
       "5   Regular Chai latte - 2.30, Regular Filter coff...    3.80         CARD   \n",
       "6   Regular Iced americano - 2.15, Regular Hot Cho...   10.75         CARD   \n",
       "7   Regular Filter coffee - 1.50, Regular Hot Choc...    8.40         CARD   \n",
       "8   Large Iced americano - 2.50, Regular Chai latt...    4.80         CARD   \n",
       "12  Regular Hot Chocolate - 1.40, Large Filter cof...    6.85         CARD   \n",
       "13  Large Speciality Tea - English breakfast - 1.6...    4.10         CARD   \n",
       "\n",
       "     card_number  \n",
       "1   5.933193e+15  \n",
       "2   2.728143e+15  \n",
       "3   2.992561e+15  \n",
       "4   8.040887e+15  \n",
       "5   2.242609e+15  \n",
       "6   1.548663e+15  \n",
       "7   6.706720e+15  \n",
       "8   2.590703e+15  \n",
       "12  4.480989e+15  \n",
       "13  3.200300e+15  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. [TEST RUN] FILTER BY COLUMN VALUE\n",
    "\n",
    "# Current Files Chesterfield or Leeds (comment one out)\n",
    "raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "# raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "column = \"payment_type\"  \n",
    "value = \"card\"\n",
    "\n",
    "contain_values = filter_by_column_value(csv,column,value)\n",
    "contain_values.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. COUNT NUMBER OF DIFFERENT VALUES IN COLUMN \n",
    "\n",
    "#### Takes: csv(absolute path to filename), column variable with the column header id,\n",
    "#### Returns: the count of the different values contained in a given column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. COUNT NUMBER OF DIFFERENT VALUES IN COLUMN \n",
    "\n",
    "def count_number_of_different_values(csv,column):\n",
    "    try:\n",
    "        count = df[column].value_counts(ascending=True)\n",
    "    except FileNotFoundError as fnfe:\n",
    "         print(f'File not found: {fnfe}')\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. [TEST RUN] COUNT NUMBER OF DIFFERENT VALUES IN COLUMN \n",
    "\n",
    "# Current Files Chesterfield or Leeds (comment one out)\n",
    "# raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "absolute_path_for_raw_data(raw_data_file) # returns csv variable with the absolut path\n",
    "\n",
    "column = \"payment_type\" \n",
    "\n",
    "result = count_number_of_different_values(csv,column)\n",
    "print(str(result))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. COUNT NUMBER OF TIMES A VALUE IS REPEATED\n",
    "# \n",
    "#### Takes: csv(absolute path to filename), column variable with the column header id, value to check\n",
    "#### Retuns: the count of the number of times a single value is repeated on a given column AS count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. COUNT NUMBER OF TIMES A VALUE IS REPEATED\n",
    "\n",
    "def count_number_of_times_a_value_is_repeated(csv,column,value):\n",
    "    try:\n",
    "        df = pd.read_csv(csv, header=None, names=columns)\n",
    "        count = df[column].value_counts()[value]\n",
    "    except FileNotFoundError as fnfe:\n",
    "         return f'File not found: {fnfe}'\n",
    "         \n",
    "    except KeyError  as kerr:\n",
    "         return f'No ocurrences of {value} in {column}' \n",
    "        \n",
    "    return f\"the value: {value} was found {str(count)} times.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the value: CASH was found 107 times.\n"
     ]
    }
   ],
   "source": [
    "# 5. [TEST RUN] COUNT NUMBER OF TIMES A VALUE IS REPEATED\n",
    "\n",
    "# Current Files Chesterfield or Leeds (comment one out)\n",
    "# raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "absolute_path_for_raw_data(raw_data_file) # returns csv variable with the absolut path\n",
    "\n",
    "column = \"payment_type\"\n",
    "value = \"CASH\" \n",
    "\n",
    "result = count_number_of_times_a_value_is_repeated(csv,column,value)\n",
    "\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. SAVE DATA FRAME TO FILE AS CSV\n",
    "\n",
    "#### Takes: path for the desired save path, newfile for the desired new file name (.csv extension added at runtime)\n",
    "#### Retuns: a csv file saved to the desired location that contains the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. SAVE DATA FRAME TO FILE AS CSV\n",
    "\n",
    "def save_df_to_csv(path,newfile):\n",
    "    try:\n",
    "        os.makedirs(path, exist_ok=True)  \n",
    "        df.to_csv(path + newfile +\".csv\", header=False)\n",
    "\n",
    "    except:\n",
    "        print(f'Saving operation could not be completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. [TEST RUN] SAVE DATA FRAME TO FILE AS CSV\n",
    "path = \"results/\" # to add folder/subfolder/ if needed\n",
    "newfile = \"newfile\"\n",
    "\n",
    "save_df_to_csv(path,newfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. SAVE DATAFRAME TO FILE AS JSON\n",
    "\n",
    "#### Takes: path for the desired save path, newfile for the desired new file name (.json extension added at runtime)\n",
    "#### Retuns: a csv file saved to the desired location that contains the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. SAVE DATAFRAME TO FILE AS JSON\n",
    "\n",
    "def save_df_to_json(path,newfile):\n",
    "    try:\n",
    "        with open(path + newfile +\".json\", 'w') as f:\n",
    "            f.write(df.to_json(orient='records', lines=True))  # add compression='gzip' to get a zip file (and change newfile extension)\n",
    "\n",
    "    except:\n",
    "        print(f'Saving operation could not be completed')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. [TEST RUN] SAVE DATAFRAME TO FILE AS JSON\n",
    "\n",
    "path = \"results/\" # to add folder/subfolder/ if needed\n",
    "newfile = \"newfile\"\n",
    "\n",
    "save_df_to_json(path,newfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET UNIQUE PRODUCTS IN ORDERS\n",
    "#### Returns a list with the number of  ocurrences for each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GET UNIQUE PRODUCTS IN ORDERS\n",
    "\n",
    "def get_unique_products_in_orders():\n",
    "    orders = df['order'].str.split(',', expand = True)\n",
    "    \n",
    "    try:\n",
    "       for item in orders:\n",
    "            result = orders[item].drop_duplicates()\n",
    "       return result \n",
    "    \n",
    "    except:\n",
    "        print(\"Operation could not be completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                   None\n",
       "50                         Regular Iced americano - 2.15\n",
       "55                               Large Chai latte - 2.60\n",
       "57      Regular Speciality Tea - English breakfast - ...\n",
       "122      Large Speciality Tea - English breakfast - 1.60\n",
       "142                          Large Iced americano - 2.50\n",
       "154                            Regular Chai latte - 2.30\n",
       "237                         Regular Filter coffee - 1.50\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [TEST RUN] GET UNIQUE PRODUCTS IN ORDERS\n",
    "\n",
    "\n",
    "# The Current Raw csv Files are: Chesterfield or Leeds (comment one out)\n",
    "raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "# raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "\n",
    "absolute_path_for_raw_data(raw_data_file)                                                           # Returns a variable called csv with the absolut path for the raw_data_file name [Run this block if fails]\n",
    "columns = ['date_time', 'location', 'full_name', 'order', 'transaction_total', 'payment_type', 'card_number']  # Headers for the DF\n",
    "df = pd.read_csv(csv, header=None, names=columns)                                                   # Creates the DF\n",
    "\n",
    "result = \"\"\n",
    "\n",
    "\n",
    "result = get_unique_products_in_orders()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Hot Chocolate \n",
      "Price:  1.70\n",
      " Regular Hot Chocolate \n",
      "Price:  1.40\n",
      " Large Chai latte \n",
      "Price:  2.60\n",
      " Regular Chai latte \n",
      "Price:  2.30\n",
      " Regular Speciality Tea - English breakfast \n",
      "Price:  1.30\n"
     ]
    }
   ],
   "source": [
    "#SEPARATE PRODUCTS IN ORDERS (string)\n",
    "\n",
    "\n",
    "# the order_products string equals to a row in df['order']\n",
    "# order_products = \"Regular Flavoured iced latte - Hazelnut - 2.75, Large Latte - 2.45\"\n",
    "order_products = \"Large Hot Chocolate - 1.70, Regular Hot Chocolate - 1.40, Large Chai latte - 2.60, Regular Chai latte - 2.30, Regular Speciality Tea - English breakfast - 1.30\"\n",
    "\n",
    "#LOGIC...\n",
    "\n",
    "# A segment (named chuck in the program) is a part of the string delimited by commas (each product in the order a dash and their price)\n",
    "# If a segment has two dashes then the first one will be the product name and the second will be the price\n",
    "# If a segment has three dashes then the first two will be the product name and the third will be the price\n",
    "\n",
    "chunks = order_products.split(',')\n",
    "\n",
    "for dashes in chunks:\n",
    "\n",
    "  if dashes.count('-') == 1:\n",
    "    print(dashes[:dashes.index(\"-\")])\n",
    "    print(f\"Price: \" + dashes.split('-')[1])\n",
    "  else: \n",
    "    stripped = dashes.split('-')[0] + \"-\" + dashes.split('-')[1]  #If the name contains a dash, combine it.\n",
    "    print(stripped)\n",
    "    print(f\"Price: \" + dashes.split('-')[2])\n",
    "\n",
    "\n",
    "# We could use this to easily separate flavors from the drinks as well  they will be in: dashes.split('-')[1] assuming the Tea Types as flavors... #\n",
    "# but that's another table to relate and probably won't be worth the trouble.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product:  Regular Speciality Tea - English breakfast \n",
      "Flavor:  Hazelnut \n",
      "Price:  2.75\n",
      "Product:  Large Latte \n",
      "Price: 2.45\n"
     ]
    }
   ],
   "source": [
    "# SEPARATE \n",
    "\n",
    "#SEPARATE PRODUCTS IN ORDERS WITH FLAVORS (string)\n",
    "\n",
    "\n",
    "# the order_products string equals to a row in df['order']\n",
    "order_products = \"Regular Flavoured iced latte - Hazelnut - 2.75, Large Latte - 2.45\"\n",
    "# order_products = \"Large Hot Chocolate - 1.70, Regular Hot Chocolate - 1.40, Large Chai latte - 2.60, Regular Chai latte - 2.30, Regular Speciality Tea - English breakfast - 1.30\"\n",
    "\n",
    "#LOGIC...\n",
    "\n",
    "# A segment (named chuck in the program) is a part of the string delimited by commas (each product in the order a dash and their price)\n",
    "# If a segment has two dashes then the first one will be the product name and the second will be the price\n",
    "# If a segment has three dashes then the first two will be the product name and the third will be the price\n",
    "\n",
    "chunks = order_products.split(',') \n",
    "\n",
    "for dashes in chunks:\n",
    "\n",
    "  if dashes.count('-') == 1:\n",
    "    product = dashes[:dashes.index(\"-\")]  \n",
    "    price = dashes.split('- ')[1]\n",
    "    \n",
    "    print(f\"Product: {product}\")\n",
    "    print(f\"Price: {price}\")\n",
    "    \n",
    "  else: \n",
    "    product = dashes.split('-')[0]\n",
    "    flavor = dashes.split('-')[1]\n",
    "    price = dashes.split('-')[2] \n",
    "    \n",
    "    print(f\"Product: {stripped}\")\n",
    "    print(f\"Flavor: {flavor}\")\n",
    "    print(f\"Price: {price}\")\n",
    " \n",
    "    \n",
    "    # With this one we can have better statistics and also count the number of flavors.\n",
    "    # REMEMBER TO REMOVE THE EMPTY SPACE AFTER THE COMMA... (TYPE(LIST)) FOR PRODUCTS AND PRICE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET NUMBER OF ROWS OF THE DATA FRAME\n",
    "#### We can use any of the following methods to get the number of rows in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET NUMBER OF ROWS OF THE DATA FRAME\n",
    "\n",
    "# len(df.index)\n",
    "# df[df.columns[0]].count()\n",
    "df.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if a file exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKS IF A FILE EXIST \n",
    "\n",
    "file_exists = os.path.exists(csv)\n",
    "print(file_exists)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SORTING THE DATE FOR POSTGRE'S [YYYY-MM-DD H:MM:SS] FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SORTING THE DATE FOR POSTGRE'S YYYY-MM-DD H:MM:SS FORMAT\n",
    "\n",
    "# Current Files are Chesterfield or Leeds (comment one out)\n",
    "# raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "\n",
    "def sort_time_to_postgre_format():\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'], dayfirst=True)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TEST RUN] SORTING THE DATE FOR POSTGRE'S YYYY-MM-DD H:MM:SS FORMAT\n",
    "\n",
    "# Current Files are Chesterfield or Leeds (comment one out)\n",
    "# raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "absolute_path_for_raw_data(raw_data_file)                                                           \n",
    "columns = ['date_time', 'location', 'full_name', 'order', 'transaction_total', 'payment_type', 'card_number']  # Headers for the DF\n",
    "# df = pd.read_csv(csv, header=None, names=columns)                                                   # Creates the DF\n",
    "\n",
    "sort_time_to_postgre_format()\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pymysql \n",
    "\n",
    "#THIS IS AN EXAMPLE OF THE CONNECT WITH DB CLASS THAT I WAS DOING BEFORE...\n",
    "#CHANGE THE VALUES IN THE TRY STATEMENT TO MAKE IT WORK WITH YOUR LOCAL DB \n",
    "\n",
    "\n",
    "### I did it with pymysql because I don't have a working postgre docker image atm) ... just replace every pymysql in the code for the other import  \n",
    "# that handles postgre... connection methods are indentical.\n",
    "\n",
    "\n",
    "#An Insert would be\n",
    "\n",
    "\n",
    "\n",
    "class WithDB():\n",
    "\n",
    "# SELECT [SQL] (2 arguments, SELECT = \"*\" TABLE = \"table_name\")\n",
    "\n",
    "    def select(select, table):\n",
    "        \n",
    "        try:\n",
    "            print(\"Connecting to DataBase...\")\n",
    "            host_name = \"localhost\"\n",
    "            database_name = \"raw_database\"\n",
    "            user_name = \"root\"\n",
    "            user_password = \"password\"\n",
    "\n",
    "            with pymysql.connect(\n",
    "                        host = host_name,\n",
    "                        database = database_name,\n",
    "                        user = user_name,\n",
    "                        password = user_password\n",
    "                    ) as connection:\n",
    "            \n",
    "                cursor = connection.cursor()\n",
    "                \n",
    "                sql = f\"SELECT {select} FROM {table}\"\n",
    "                cursor.execute(sql)\n",
    "                table_data = cursor.fetchall()\n",
    "                print(table_data)\n",
    "                connection.commit()\n",
    "        \n",
    "        except Exception as ex:\n",
    "            print(\"Failed to open connection, please make sure DB is Running\")\n",
    "            \n",
    "\n",
    "def insert(row, table,columns):\n",
    "        \n",
    "        try:\n",
    "            print(\"Connecting to DataBase...\")\n",
    "            host_name = \"localhost\"\n",
    "            database_name = \"raw_database\"\n",
    "            user_name = \"root\"\n",
    "            user_password = \"password\"\n",
    "\n",
    "            with pymysql.connect(\n",
    "                        host = host_name,\n",
    "                        database = database_name,\n",
    "                        user = user_name,\n",
    "                        password = user_password\n",
    "                    ) as connection:\n",
    "            \n",
    "                cursor = connection.cursor()\n",
    "                \n",
    "                sql = f\"INSERT INTO {table} {columns} VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "                values = columns\n",
    "                \n",
    "                cursor.execute(sql,values)\n",
    "                table_data = cursor.fetchall()\n",
    "                print(table_data)\n",
    "                connection.commit()\n",
    "        \n",
    "        except Exception as ex:\n",
    "            print(\"Failed to open connection, please make sure DB is Running\")\n",
    "    \n",
    "\n",
    "#The whole idea of the \"WithDB\" class is that I can be called like this for different scenarios\n",
    "WithDB.select(\"*\",\"raw_order\")\n",
    "\n",
    "\n",
    "\n",
    "# A row example.... edit for one value if it fails so you can check how it works\n",
    "row = '(25/08/2021 09:00, Chesterfield,Richard Copeland, \"Regular Flavoured iced latte - Hazelnut - 2.75, Large Latte - 2.45\", 5.2, CARD, 5494173772652516)'\n",
    "# the target table \n",
    "table = \"raw_order\"\n",
    "#your table db columns  \n",
    "columns = '(raw_id,date,location,full_name,orders,transaction_total,payment_type,card_number)'\n",
    "\n",
    "#then you can call it like this:\n",
    "WithDB.insert(row,table,columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DATA PIPELINE ROUTE... (please disable the markdown... didn't want to spend more time formatting this tbh)\n",
    "\n",
    "1. READ CSV FILE AS DATA FRAME  [DONE]\n",
    "\n",
    "EXAMPLE OF A ROW AT THIS POINT: <br>\n",
    "25/08/2021 09:00, Chesterfield,Richard Copeland, \"Regular Flavoured iced latte - Hazelnut - 2.75, Large Latte - 2.45\", 5.2, CARD, 5494173772652516\n",
    "\n",
    "2. REMOVE THE COLUMNS NAME, CARD NUMBER [DONE]\n",
    "\n",
    "EXAMPLE OF A ROW AT THIS POINT:  <br>\n",
    "25/08/2021 09:00, Chesterfield, \"Regular Flavoured iced latte - Hazelnut - 2.75, Large Latte - 2.45\", 5.2, CARD\n",
    "\n",
    "\n",
    "3. CHANGE DATE FORMAT TO POSTQGRE'S FORMAT [DONE]\n",
    "\n",
    "EXAMPLE OF A ROW AT THIS POINT:  <br>\n",
    "\n",
    "2021/08/25 09:00, Chesterfield,\"Regular Flavoured iced latte - Hazelnut - 2.75, Large Latte - 2.45\", 5.2, CARD\n",
    "\n",
    "\n",
    "4. NORMALISE LOCATION AND PAYMENT_TYPE (A FUNCTION THAT CONNECTS TO DB AND CHECK IF THE LOCATION EXISTS RETURNING THE ID, IF NOT ADDS IT. SHOULD BE CREATED)\n",
    "\n",
    "EXAMPLE OF A ROW AT THIS POINT:  <br>\n",
    "\n",
    "\n",
    "2021/08/25 09:00, 1 ,\"Regular Flavoured iced latte - Hazelnut - 2.75, Large Latte - 2.45\", 5.2, 2\n",
    "\n",
    "\n",
    "FOR PAYMENT TYPE\n",
    "\n",
    "and a sql table named [payment_type] with \n",
    "\n",
    "payment_type_id, payment_type_name\n",
    "        1               CASH\n",
    "        2               CARD\n",
    "\n",
    "A function that replaces CASH for 1 and CARD for 2 on each row needs to be done.\n",
    "\n",
    "----\n",
    "\n",
    "FOR LOCATION\n",
    "\n",
    "A function that reads all the locations,location_id from the database as a dict needs to be implemented \n",
    "\n",
    "then we check the location column for each row against the dict... if it exist we will replace the name with the location_id number\n",
    "if it doesn't, we have to insert the new location into the locations table, return the location_id for that insert and add it to the dict and continue the normalisation\n",
    "\n",
    "[Location]\n",
    "\n",
    "location_id   location_name\n",
    "        1        Chesterfield\n",
    "        2        Leeds\n",
    "\n",
    "NOTE: maybe we should do this with payment_type aswell ... just in case we encounter a customer that pays with something like a CHEQUE or GIFTCARD later on\n",
    "\n",
    "\n",
    "5. EXTRACT PRODUCTS FROM EACH ORDER AND ADD THEM TO A SEPARATE DATA FRAME WITH [ORDER_ID] [PRODUCT_NAME]\n",
    "\n",
    "EXAMPLE OF A ROW AT THIS POINT: <br>\n",
    "\n",
    "2021/08/25 09:00, 1 ,1, 5.2, 2  <br>\n",
    "\n",
    "AND THIS WILL BE THE ORDER FOR SAID ROW  <br>\n",
    "\n",
    "Regular Flavoured iced latte - Hazelnut - 2.75\n",
    "Large Latte - 2.45\n",
    "\n",
    "\n",
    "6. NORMALISE ORDER PRODUCTS (A FUNCTION THAT CONNECTS TO DB AND CHECK IF THE PRODUCT EXISTS RETURNING THE ID, IF NOT ADDS IT. SHOULD BE CREATED)\n",
    "\n",
    "EXAMPLE OF A ROW AT THIS POINT: <br>\n",
    "\n",
    "2021/08/25 09:00, 1 ,1, 5.2, 2 <br>\n",
    "\n",
    "A table [order_products] will have the order_id and the product id <br>\n",
    "\n",
    "order_id  product_id\n",
    "    1        1\n",
    "    1        2\n",
    "\n",
    "A table [products] should be created \n",
    "\n",
    "product_id    product_name                               product_price\n",
    "    1         Regular Flavoured iced latte - Hazelnut          2.75\n",
    "\n",
    "all the relations should be made between those tables [this is done already in our schema]\n",
    "\n",
    "NOTE: Just in case... the function that I made can separate Hazelnut to \"flavors\" if we need that bit later on\n",
    "\n",
    "\n",
    "7. LOAD INTO DB\n",
    "\n",
    "tbh... I would prefer to insert the bits into tables after each step and leave the orders one for last ... \n",
    "but if we're not meant to do it that way, then we will load everything here...\n",
    "\n",
    "We'll use the class WithDB for this.... \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
