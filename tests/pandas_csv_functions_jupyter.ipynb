{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Pandas Functions\n",
    "\n",
    "Please run the imports first and keep in mind that some blocks require an absolute path csv and a DF loaded to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd                                                                                 # Loads Pandas package\n",
    "import os                                                                                           # For OS related paths "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. ABSOLUTE PATH FOR RAW DATA   \n",
    "\n",
    "##### Takes: raw_data_name that should be the name of one of the csv's files \n",
    "##### Returns: a variable named csv containing the absolute path to the raw_data subfolder folder plus the name of the file in raw_data_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ABSOLUTE PATH FOR RAW DATA   \n",
    "\n",
    "def absolute_path_for_raw_data(raw_data_file):\n",
    "    abspath = os.path.abspath(\"../raw_data\")\n",
    "    global csv\n",
    "    csv = abspath + \"\\\\\" + raw_data_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. [TEST RUN] ABSOLUTE PATH FOR RAW DATA\n",
    "\n",
    "# Current Files are Chesterfield or Leeds (comment one out)\n",
    "# raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "absolute_path_for_raw_data(raw_data_file)\n",
    "print(csv) # Prints the newly formed absolute path with the raw_data_file name given.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #LOADS (EXTRACT) DATAFRAME\n",
    "\n",
    "#### Loads the df with headers .... just put outside any function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADS (EXTRACT) DATAFRAME\n",
    "columns = ['date_time', 'Location', 'full_name', 'order', 'amount', 'payment_type', 'card_number']  # Headers for the orders csv files\n",
    "df = pd.read_csv(csv, header=None, names=columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SANITISE CSV (Remove given Columns) \n",
    "\n",
    "#### Takes: csv(absolute path to filename), and a list of columns to be dropped on a variable called sanatise_these_columns\n",
    "#### Returns: the sanitised dataframe as df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SANITISE CSV (Remove given Columns) \n",
    "\n",
    "def sanitise_csv(csv,sanitise_these_columns):\n",
    "    try:\n",
    "        global df # as Global to be able to print it outside the function\n",
    "        sanatisedf = df.drop(columns=sanitise_these_columns)\n",
    "    except FileNotFoundError as fnfe:\n",
    "        print(f'File not found: {fnfe}')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. [TEST RUN] SANITISE CSV \n",
    "\n",
    "# Current Files Chesterfield or Leeds (comment one out)\n",
    "# raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "columns = ['date_time', 'Location', 'full_name', 'order', 'amount', 'payment_type', 'card_number']  # Headers for the orders csv files\n",
    "\n",
    "absolute_path_for_raw_data(raw_data_file)  # returns csv variable with the absolute path\n",
    " \n",
    "sanitise_these_columns = ['full_name', 'card_number']\n",
    "\n",
    "result = sanitise_csv(csv,sanitise_these_columns)\n",
    "result.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. FILTER BY COLUMN VALUE \n",
    "\n",
    "#### Takes: csv(absolute path to filename), column variable with the column header id, value to check\n",
    "#### Returns: the rows in which a column contains a given value AS contain_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. FILTER BY COLUMN VALUE \n",
    "\n",
    "def filter_by_column_value(csv,column,value):\n",
    "    try:\n",
    "        global contain_values # as Global to be able to print it outside the function\n",
    "        contain_values = df[df[column].str.contains(value.upper())]\n",
    "    except FileNotFoundError as fnfe:\n",
    "        print(f'File not found: {fnfe}')\n",
    "\n",
    "    return contain_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. [TEST RUN] FILTER BY COLUMN VALUE\n",
    "\n",
    "# Current Files Chesterfield or Leeds (comment one out)\n",
    "raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "# raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "column = \"payment_type\"  \n",
    "value = \"card\"\n",
    "\n",
    "filter_by_column_value(csv,column,value)\n",
    "contain_values.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. COUNT NUMBER OF DIFFERENT VALUES IN COLUMN \n",
    "\n",
    "#### Takes: csv(absolute path to filename), column variable with the column header id,\n",
    "#### Returns: the count of the different values contained in a given column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. COUNT NUMBER OF DIFFERENT VALUES IN COLUMN \n",
    "\n",
    "def count_number_of_different_values(csv,column):\n",
    "    try:\n",
    "        count = df[column].value_counts(ascending=True)\n",
    "    except FileNotFoundError as fnfe:\n",
    "         print(f'File not found: {fnfe}')\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. [TEST RUN] COUNT NUMBER OF DIFFERENT VALUES IN COLUMN \n",
    "\n",
    "# Current Files Chesterfield or Leeds (comment one out)\n",
    "# raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "absolute_path_for_raw_data(raw_data_file) # returns csv variable with the absolut path\n",
    "\n",
    "column = \"payment_type\" \n",
    "\n",
    "result = count_number_of_different_values(csv,column)\n",
    "print(str(result))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. COUNT NUMBER OF TIMES A VALUE IS REPEATED\n",
    "# \n",
    "#### Takes: csv(absolute path to filename), column variable with the column header id, value to check\n",
    "#### Retuns: the count of the number of times a single value is repeated on a given column AS count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. COUNT NUMBER OF TIMES A VALUE IS REPEATED\n",
    "\n",
    "def count_number_of_times_a_value_is_repeated(csv,column,value):\n",
    "    try:\n",
    "        df = pd.read_csv(csv, header=None, names=columns)\n",
    "        #global count # as Global to be able to print it outside the function\n",
    "        count = df[column].value_counts()[value]\n",
    "    except FileNotFoundError as fnfe:\n",
    "         return f'File not found: {fnfe}'\n",
    "         \n",
    "    except KeyError  as kerr:\n",
    "         return f'No ocurrences of {value} in {column}' \n",
    "        \n",
    "    return f\"the value: {value} was found {str(count)} times.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. [TEST RUN] COUNT NUMBER OF TIMES A VALUE IS REPEATED\n",
    "\n",
    "# Current Files Chesterfield or Leeds (comment one out)\n",
    "# raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "absolute_path_for_raw_data(raw_data_file) # returns csv variable with the absolut path\n",
    "\n",
    "column = \"payment_type\"\n",
    "value = \"CASH\" \n",
    "\n",
    "result = count_number_of_times_a_value_is_repeated(csv,column,value)\n",
    "\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. SAVE DATA FRAME TO FILE AS CSV\n",
    "\n",
    "#### Takes: path for the desired save path, newfile for the desired new file name (.csv extension added at runtime)\n",
    "#### Retuns: a csv file saved to the desired location that contains the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. SAVE DATA FRAME TO FILE AS CSV\n",
    "\n",
    "def save_df_to_csv(path,newfile):\n",
    "    try:\n",
    "        os.makedirs(path, exist_ok=True)  \n",
    "        df.to_csv(path + newfile +\".csv\", header=False)\n",
    "\n",
    "    except:\n",
    "        print(f'Saving operation could not be completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. [TEST RUN] SAVE DATA FRAME TO FILE AS CSV\n",
    "path = \"results/\" # to add folder/subfolder/ if needed\n",
    "newfile = \"newfile\"\n",
    "\n",
    "save_df_to_csv(path,newfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. SAVE DATAFRAME TO FILE AS JSON\n",
    "\n",
    "#### Takes: path for the desired save path, newfile for the desired new file name (.json extension added at runtime)\n",
    "#### Retuns: a csv file saved to the desired location that contains the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. SAVE DATAFRAME TO FILE AS JSON\n",
    "\n",
    "def save_df_to_json(path,newfile):\n",
    "    try:\n",
    "        with open(path + newfile +\".json\", 'w') as f:\n",
    "            f.write(df.to_json(orient='records', lines=True))  # add compression='gzip' to get a zip file (and change newfile extension)\n",
    "\n",
    "    except:\n",
    "        print(f'Saving operation could not be completed')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. [TEST RUN] SAVE DATAFRAME TO FILE AS JSON\n",
    "\n",
    "path = \"results/\" # to add folder/subfolder/ if needed\n",
    "newfile = \"newfile\"\n",
    "\n",
    "save_df_to_json(path,newfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET UNIQUE PRODUCTS IN ORDERS\n",
    "#### Returns a list with the number of  ocurrences for each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GET UNIQUE PRODUCTS IN ORDERS\n",
    "\n",
    "def get_unique_products_in_orders():\n",
    "    orders = df['order'].str.split(',', expand = True)\n",
    "    \n",
    "    try:\n",
    "       for item in orders:\n",
    "            global result\n",
    "            result = orders[item].drop_duplicates()\n",
    "       return result \n",
    "    \n",
    "    except:\n",
    "        print(\"Operation could not be completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TEST RUN] GET UNIQUE PRODUCTS IN ORDERS\n",
    "\n",
    "\n",
    "# The Current Raw csv Files are: Chesterfield or Leeds (comment one out)\n",
    "raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "# raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "\n",
    "absolute_path_for_raw_data(raw_data_file)                                                           # Returns a variable called csv with the absolut path for the raw_data_file name [Run this block if fails]\n",
    "columns = ['date_time', 'Location', 'full_name', 'order', 'amount', 'payment_type', 'card_number']  # Headers for the DF\n",
    "df = pd.read_csv(csv, header=None, names=columns)                                                   # Creates the DF\n",
    "\n",
    "result = \"\"\n",
    "\n",
    "\n",
    "get_unique_products_in_orders()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEPARATE PRODUCTS IN ORDERS (string)\n",
    "\n",
    "\n",
    "# the order_products string equals to a row in df['order']\n",
    "# order_products = \"Regular Flavoured iced latte - Hazelnut - 2.75, Large Latte - 2.45\"\n",
    "order_products = \"Large Hot Chocolate - 1.70, Regular Hot Chocolate - 1.40, Large Chai latte - 2.60, Regular Chai latte - 2.30, Regular Speciality Tea - English breakfast - 1.30\"\n",
    "\n",
    "#LOGIC...\n",
    "\n",
    "# A segment (named chuck in the program) is a part of the string delimited by commas (each product in the order a dash and their price)\n",
    "# If a segment has two dashes then the first one will be the product name and the second will be the price\n",
    "# If a segment has three dashes then the first two will be the product name and the third will be the price\n",
    "\n",
    "chunks = order_products.split(',')\n",
    "\n",
    "for dashes in chunks:\n",
    "\n",
    "  if dashes.count('-') == 1:\n",
    "    print(dashes[:dashes.index(\"-\")])\n",
    "    print(f\"Price: \" + dashes.split('-')[1])\n",
    "  else: \n",
    "    stripped = dashes.split('-')[0] + \"-\" + dashes.split('-')[1]  #If the name contains a dash, combine it.\n",
    "    print(stripped)\n",
    "    print(f\"Price: \" + dashes.split('-')[2])\n",
    "\n",
    "\n",
    "# We could use this to easily separate flavors from the drinks as well  they will be in: dashes.split('-')[1] assuming the Tea Types as flavors... #\n",
    "# but that's another table to relate and probably won't be worth the trouble.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEPARATE \n",
    "\n",
    "#SEPARATE PRODUCTS IN ORDERS WITH FLAVORS (string)\n",
    "\n",
    "\n",
    "# the order_products string equals to a row in df['order']\n",
    "order_products = \"Regular Flavoured iced latte - Hazelnut - 2.75, Large Latte - 2.45\"\n",
    "# order_products = \"Large Hot Chocolate - 1.70, Regular Hot Chocolate - 1.40, Large Chai latte - 2.60, Regular Chai latte - 2.30, Regular Speciality Tea - English breakfast - 1.30\"\n",
    "\n",
    "#LOGIC...\n",
    "\n",
    "# A segment (named chuck in the program) is a part of the string delimited by commas (each product in the order a dash and their price)\n",
    "# If a segment has two dashes then the first one will be the product name and the second will be the price\n",
    "# If a segment has three dashes then the first two will be the product name and the third will be the price\n",
    "\n",
    "chunks = order_products.split(',') \n",
    "\n",
    "for dashes in chunks:\n",
    "\n",
    "  if dashes.count('-') == 1:\n",
    "    product = dashes[:dashes.index(\"-\")]  \n",
    "    price = dashes.split('- ')[1]\n",
    "    \n",
    "    print(f\"Product: {product}\")\n",
    "    print(f\"Price: {price}\")\n",
    "    \n",
    "  else: \n",
    "    product = dashes.split('-')[0]\n",
    "    flavor = dashes.split('-')[1]\n",
    "    price = dashes.split('-')[2] \n",
    "    \n",
    "    print(f\"Product: {stripped}\")\n",
    "    print(f\"Flavor: {flavor}\")\n",
    "    print(f\"Price: {price}\")\n",
    " \n",
    "    \n",
    "    # With this one we can have better statistics and also count the number of flavors.\n",
    "    # REMEMBER TO REMOVE THE EMPTY SPACE AFTER THE COMMA... (TYPE(LIST)) FOR PRODUCTS AND PRICE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET NUMBER OF ROWS OF THE DATA FRAME\n",
    "#### We can use any of the following methods to get the number of rows in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET NUMBER OF ROWS OF THE DATA FRAME\n",
    "\n",
    "# len(df.index)\n",
    "# df[df.columns[0]].count()\n",
    "df.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if a file exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKS IF A FILE EXIST \n",
    "\n",
    "file_exists = os.path.exists(csv)\n",
    "print(file_exists)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SORTING THE DATE FOR POSTGRE'S [YYYY-MM-DD H:MM:SS] FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SORTING THE DATE FOR POSTGRE'S YYYY-MM-DD H:MM:SS FORMAT\n",
    "\n",
    "# Current Files are Chesterfield or Leeds (comment one out)\n",
    "# raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "\n",
    "def sort_time_to_postgre_format():\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'], dayfirst=True)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TEST RUN] SORTING THE DATE FOR POSTGRE'S YYYY-MM-DD H:MM:SS FORMAT\n",
    "\n",
    "# Current Files are Chesterfield or Leeds (comment one out)\n",
    "# raw_data_file = \"chesterfield_25-08-2021_09-00-00.csv\"\n",
    "raw_data_file = \"leeds_01-01-2020_09-00-00.csv\"\n",
    "\n",
    "absolute_path_for_raw_data(raw_data_file)                                                           \n",
    "columns = ['date_time', 'Location', 'full_name', 'order', 'amount', 'payment_type', 'card_number']  # Headers for the DF\n",
    "# df = pd.read_csv(csv, header=None, names=columns)                                                   # Creates the DF\n",
    "\n",
    "sort_time_to_postgre_format()\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pymysql \n",
    "\n",
    "#THIS IS AN EXAMPLE OF CONNECT OF THE CONNECT WITH DB CLASS\n",
    "#CHANGE THE VALUES IN THE TRY STATEMENT TO MAKE IT WORK WITH YOUR LOCAL DB \n",
    "\n",
    "\n",
    "### I did it with pymysql because I don't have a working postgre docker image atm) ... just replace every pymysql in the code for the other import  \n",
    "# that handles postgre... connection methods are indentical.\n",
    "\n",
    "\n",
    "class WithDB():\n",
    "\n",
    "# SELECT [SQL] (2 arguments, SELECT = \"*\" TABLE = \"table_name\")\n",
    "\n",
    "    def select(select, table):\n",
    "        \n",
    "        try:\n",
    "            print(\"Connecting to DataBase...\")\n",
    "            host_name = \"localhost\"\n",
    "            database_name = \"raw_database\"\n",
    "            user_name = \"root\"\n",
    "            user_password = \"password\"\n",
    "\n",
    "            with pymysql.connect(\n",
    "                        host = host_name,\n",
    "                        database = database_name,\n",
    "                        user = user_name,\n",
    "                        password = user_password\n",
    "                    ) as connection:\n",
    "            \n",
    "                cursor = connection.cursor()\n",
    "                \n",
    "                sql = f\"SELECT {select} FROM {table}\"\n",
    "                cursor.execute(sql)\n",
    "                table_data = cursor.fetchall()\n",
    "                print(table_data)\n",
    "                connection.commit()\n",
    "        \n",
    "        except Exception as ex:\n",
    "            print(\"Failed to open connection, please make sure DB is Running\")\n",
    "            \n",
    "\n",
    "#The whole idea of the \"WithDB\" class is that I can be called like this for different scenarios\n",
    "WithDB.select(\"*\",\"raw_order\")\n",
    "\n",
    "#An Insert would be\n",
    "#WithDB.insert(data,table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
